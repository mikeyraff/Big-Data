{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ac57a541-fa7d-450f-9819-d2f89638d5b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Datacenter: datacenter1\n",
      "=======================\n",
      "Status=Up/Down\n",
      "|/ State=Normal/Leaving/Joining/Moving\n",
      "--  Address     Load        Tokens  Owns (effective)  Host ID                               Rack \n",
      "UN  172.22.0.2  151.38 KiB  16      80.4%             98d6bc19-ebce-434a-bbf4-b04836f5f591  rack1\n",
      "UN  172.22.0.3  151.71 KiB  16      60.1%             3b2f5122-1b49-46b7-a52a-479b2f479a25  rack1\n",
      "UN  172.22.0.4  151.28 KiB  16      59.5%             6b9500ae-0149-45ca-af77-9930922a6587  rack1\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!nodetool status"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ca07af53-f82f-4999-803d-8f65b2479f28",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<cassandra.cluster.ResultSet at 0x75c0f82f4e50>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from cassandra.cluster import Cluster\n",
    "cluster = Cluster(['p6-db-1', 'p6-db-2', 'p6-db-3'])\n",
    "cass = cluster.connect()\n",
    "\n",
    "cass.execute(\"DROP KEYSPACE IF EXISTS weather\")\n",
    "\n",
    "cass.execute(\"\"\"\n",
    "CREATE KEYSPACE weather WITH replication = {\n",
    "    'class': 'SimpleStrategy',\n",
    "    'replication_factor': 3\n",
    "}\n",
    "\"\"\")\n",
    "\n",
    "cass.execute(\"\"\"\n",
    "CREATE TYPE weather.station_record (\n",
    "    tmin int,\n",
    "    tmax int\n",
    ")\n",
    "\"\"\")\n",
    "\n",
    "cass.execute(\"\"\"\n",
    "CREATE TABLE weather.stations (\n",
    "    id text,\n",
    "    name text static,\n",
    "    date date,\n",
    "    record weather.station_record,\n",
    "    PRIMARY KEY (id, date)\n",
    ") WITH CLUSTERING ORDER BY (date ASC)\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4ff0ffc6-2261-409e-a3b2-876752f4b640",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"CREATE TABLE weather.stations (\\n    id text,\\n    date date,\\n    name text static,\\n    record station_record,\\n    PRIMARY KEY (id, date)\\n) WITH CLUSTERING ORDER BY (date ASC)\\n    AND additional_write_policy = '99p'\\n    AND bloom_filter_fp_chance = 0.01\\n    AND caching = {'keys': 'ALL', 'rows_per_partition': 'NONE'}\\n    AND cdc = false\\n    AND comment = ''\\n    AND compaction = {'class': 'org.apache.cassandra.db.compaction.SizeTieredCompactionStrategy', 'max_threshold': '32', 'min_threshold': '4'}\\n    AND compression = {'chunk_length_in_kb': '16', 'class': 'org.apache.cassandra.io.compress.LZ4Compressor'}\\n    AND memtable = 'default'\\n    AND crc_check_chance = 1.0\\n    AND default_time_to_live = 0\\n    AND extensions = {}\\n    AND gc_grace_seconds = 864000\\n    AND max_index_interval = 2048\\n    AND memtable_flush_period_in_ms = 0\\n    AND min_index_interval = 128\\n    AND read_repair = 'BLOCKING'\\n    AND speculative_retry = '99p';\""
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#q1\n",
    "create_statement = cass.execute(\"DESCRIBE TABLE weather.stations\").one().create_statement\n",
    "create_statement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "be0faa76-2897-4889-883f-9ade6d7caeea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ":: loading settings :: url = jar:file:/usr/local/lib/python3.10/dist-packages/pyspark/jars/ivy-2.5.1.jar!/org/apache/ivy/core/settings/ivysettings.xml\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ivy Default Cache set to: /root/.ivy2/cache\n",
      "The jars for the packages stored in: /root/.ivy2/jars\n",
      "com.datastax.spark#spark-cassandra-connector_2.12 added as a dependency\n",
      ":: resolving dependencies :: org.apache.spark#spark-submit-parent-f4776a6d-abbb-4f4d-923a-30c58a5c1bc7;1.0\n",
      "\tconfs: [default]\n",
      "\tfound com.datastax.spark#spark-cassandra-connector_2.12;3.4.0 in central\n",
      "\tfound com.datastax.spark#spark-cassandra-connector-driver_2.12;3.4.0 in central\n",
      "\tfound com.datastax.oss#java-driver-core-shaded;4.13.0 in central\n",
      "\tfound com.datastax.oss#native-protocol;1.5.0 in central\n",
      "\tfound com.datastax.oss#java-driver-shaded-guava;25.1-jre-graal-sub-1 in central\n",
      "\tfound com.typesafe#config;1.4.1 in central\n",
      "\tfound org.slf4j#slf4j-api;1.7.26 in central\n",
      "\tfound io.dropwizard.metrics#metrics-core;4.1.18 in central\n",
      "\tfound org.hdrhistogram#HdrHistogram;2.1.12 in central\n",
      "\tfound org.reactivestreams#reactive-streams;1.0.3 in central\n",
      "\tfound com.github.stephenc.jcip#jcip-annotations;1.0-1 in central\n",
      "\tfound com.github.spotbugs#spotbugs-annotations;3.1.12 in central\n",
      "\tfound com.google.code.findbugs#jsr305;3.0.2 in central\n",
      "\tfound com.datastax.oss#java-driver-mapper-runtime;4.13.0 in central\n",
      "\tfound com.datastax.oss#java-driver-query-builder;4.13.0 in central\n",
      "\tfound org.apache.commons#commons-lang3;3.10 in central\n",
      "\tfound com.thoughtworks.paranamer#paranamer;2.8 in central\n",
      "\tfound org.scala-lang#scala-reflect;2.12.11 in central\n",
      "downloading https://repo1.maven.org/maven2/com/datastax/spark/spark-cassandra-connector_2.12/3.4.0/spark-cassandra-connector_2.12-3.4.0.jar ...\n",
      "\t[SUCCESSFUL ] com.datastax.spark#spark-cassandra-connector_2.12;3.4.0!spark-cassandra-connector_2.12.jar (159ms)\n",
      "downloading https://repo1.maven.org/maven2/com/datastax/spark/spark-cassandra-connector-driver_2.12/3.4.0/spark-cassandra-connector-driver_2.12-3.4.0.jar ...\n",
      "\t[SUCCESSFUL ] com.datastax.spark#spark-cassandra-connector-driver_2.12;3.4.0!spark-cassandra-connector-driver_2.12.jar (102ms)\n",
      "downloading https://repo1.maven.org/maven2/com/datastax/oss/java-driver-core-shaded/4.13.0/java-driver-core-shaded-4.13.0.jar ...\n",
      "\t[SUCCESSFUL ] com.datastax.oss#java-driver-core-shaded;4.13.0!java-driver-core-shaded.jar (463ms)\n",
      "downloading https://repo1.maven.org/maven2/com/datastax/oss/java-driver-mapper-runtime/4.13.0/java-driver-mapper-runtime-4.13.0.jar ...\n",
      "\t[SUCCESSFUL ] com.datastax.oss#java-driver-mapper-runtime;4.13.0!java-driver-mapper-runtime.jar(bundle) (31ms)\n",
      "downloading https://repo1.maven.org/maven2/org/apache/commons/commons-lang3/3.10/commons-lang3-3.10.jar ...\n",
      "\t[SUCCESSFUL ] org.apache.commons#commons-lang3;3.10!commons-lang3.jar (57ms)\n",
      "downloading https://repo1.maven.org/maven2/com/thoughtworks/paranamer/paranamer/2.8/paranamer-2.8.jar ...\n",
      "\t[SUCCESSFUL ] com.thoughtworks.paranamer#paranamer;2.8!paranamer.jar(bundle) (28ms)\n",
      "downloading https://repo1.maven.org/maven2/org/scala-lang/scala-reflect/2.12.11/scala-reflect-2.12.11.jar ...\n",
      "\t[SUCCESSFUL ] org.scala-lang#scala-reflect;2.12.11!scala-reflect.jar (160ms)\n",
      "downloading https://repo1.maven.org/maven2/com/datastax/oss/native-protocol/1.5.0/native-protocol-1.5.0.jar ...\n",
      "\t[SUCCESSFUL ] com.datastax.oss#native-protocol;1.5.0!native-protocol.jar(bundle) (38ms)\n",
      "downloading https://repo1.maven.org/maven2/com/datastax/oss/java-driver-shaded-guava/25.1-jre-graal-sub-1/java-driver-shaded-guava-25.1-jre-graal-sub-1.jar ...\n",
      "\t[SUCCESSFUL ] com.datastax.oss#java-driver-shaded-guava;25.1-jre-graal-sub-1!java-driver-shaded-guava.jar (105ms)\n",
      "downloading https://repo1.maven.org/maven2/com/typesafe/config/1.4.1/config-1.4.1.jar ...\n",
      "\t[SUCCESSFUL ] com.typesafe#config;1.4.1!config.jar(bundle) (32ms)\n",
      "downloading https://repo1.maven.org/maven2/org/slf4j/slf4j-api/1.7.26/slf4j-api-1.7.26.jar ...\n",
      "\t[SUCCESSFUL ] org.slf4j#slf4j-api;1.7.26!slf4j-api.jar (26ms)\n",
      "downloading https://repo1.maven.org/maven2/io/dropwizard/metrics/metrics-core/4.1.18/metrics-core-4.1.18.jar ...\n",
      "\t[SUCCESSFUL ] io.dropwizard.metrics#metrics-core;4.1.18!metrics-core.jar(bundle) (30ms)\n",
      "downloading https://repo1.maven.org/maven2/org/hdrhistogram/HdrHistogram/2.1.12/HdrHistogram-2.1.12.jar ...\n",
      "\t[SUCCESSFUL ] org.hdrhistogram#HdrHistogram;2.1.12!HdrHistogram.jar(bundle) (34ms)\n",
      "downloading https://repo1.maven.org/maven2/org/reactivestreams/reactive-streams/1.0.3/reactive-streams-1.0.3.jar ...\n",
      "\t[SUCCESSFUL ] org.reactivestreams#reactive-streams;1.0.3!reactive-streams.jar (26ms)\n",
      "downloading https://repo1.maven.org/maven2/com/github/stephenc/jcip/jcip-annotations/1.0-1/jcip-annotations-1.0-1.jar ...\n",
      "\t[SUCCESSFUL ] com.github.stephenc.jcip#jcip-annotations;1.0-1!jcip-annotations.jar (26ms)\n",
      "downloading https://repo1.maven.org/maven2/com/github/spotbugs/spotbugs-annotations/3.1.12/spotbugs-annotations-3.1.12.jar ...\n",
      "\t[SUCCESSFUL ] com.github.spotbugs#spotbugs-annotations;3.1.12!spotbugs-annotations.jar (28ms)\n",
      "downloading https://repo1.maven.org/maven2/com/google/code/findbugs/jsr305/3.0.2/jsr305-3.0.2.jar ...\n",
      "\t[SUCCESSFUL ] com.google.code.findbugs#jsr305;3.0.2!jsr305.jar (27ms)\n",
      "downloading https://repo1.maven.org/maven2/com/datastax/oss/java-driver-query-builder/4.13.0/java-driver-query-builder-4.13.0.jar ...\n",
      "\t[SUCCESSFUL ] com.datastax.oss#java-driver-query-builder;4.13.0!java-driver-query-builder.jar(bundle) (34ms)\n",
      ":: resolution report :: resolve 5829ms :: artifacts dl 1449ms\n",
      "\t:: modules in use:\n",
      "\tcom.datastax.oss#java-driver-core-shaded;4.13.0 from central in [default]\n",
      "\tcom.datastax.oss#java-driver-mapper-runtime;4.13.0 from central in [default]\n",
      "\tcom.datastax.oss#java-driver-query-builder;4.13.0 from central in [default]\n",
      "\tcom.datastax.oss#java-driver-shaded-guava;25.1-jre-graal-sub-1 from central in [default]\n",
      "\tcom.datastax.oss#native-protocol;1.5.0 from central in [default]\n",
      "\tcom.datastax.spark#spark-cassandra-connector-driver_2.12;3.4.0 from central in [default]\n",
      "\tcom.datastax.spark#spark-cassandra-connector_2.12;3.4.0 from central in [default]\n",
      "\tcom.github.spotbugs#spotbugs-annotations;3.1.12 from central in [default]\n",
      "\tcom.github.stephenc.jcip#jcip-annotations;1.0-1 from central in [default]\n",
      "\tcom.google.code.findbugs#jsr305;3.0.2 from central in [default]\n",
      "\tcom.thoughtworks.paranamer#paranamer;2.8 from central in [default]\n",
      "\tcom.typesafe#config;1.4.1 from central in [default]\n",
      "\tio.dropwizard.metrics#metrics-core;4.1.18 from central in [default]\n",
      "\torg.apache.commons#commons-lang3;3.10 from central in [default]\n",
      "\torg.hdrhistogram#HdrHistogram;2.1.12 from central in [default]\n",
      "\torg.reactivestreams#reactive-streams;1.0.3 from central in [default]\n",
      "\torg.scala-lang#scala-reflect;2.12.11 from central in [default]\n",
      "\torg.slf4j#slf4j-api;1.7.26 from central in [default]\n",
      "\t---------------------------------------------------------------------\n",
      "\t|                  |            modules            ||   artifacts   |\n",
      "\t|       conf       | number| search|dwnlded|evicted|| number|dwnlded|\n",
      "\t---------------------------------------------------------------------\n",
      "\t|      default     |   18  |   18  |   18  |   0   ||   18  |   18  |\n",
      "\t---------------------------------------------------------------------\n",
      ":: retrieving :: org.apache.spark#spark-submit-parent-f4776a6d-abbb-4f4d-923a-30c58a5c1bc7\n",
      "\tconfs: [default]\n",
      "\t18 artifacts copied, 0 already retrieved (18067kB/93ms)\n",
      "24/04/17 00:20:05 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "spark = (SparkSession.builder\n",
    "         .appName(\"p6\")\n",
    "         .config('spark.jars.packages', 'com.datastax.spark:spark-cassandra-connector_2.12:3.4.0')\n",
    "         .config(\"spark.sql.extensions\", \"com.datastax.spark.connector.CassandraSparkExtensions\")\n",
    "         .getOrCreate())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9f83124a-e369-4c81-a044-85fde25277bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df = spark.read.text(\"ghcnd-stations.txt\")\n",
    "\n",
    "stations_df = df.select(\n",
    "    df.value.substr(1, 11).alias(\"id\"),\n",
    "    df.value.substr(39, 2).alias(\"state\"),\n",
    "    df.value.substr(42, 30).alias(\"name\")\n",
    ")\n",
    "\n",
    "wi_stations = stations_df.filter(stations_df.state == \"WI\")\n",
    "\n",
    "local_stations = wi_stations.collect()\n",
    "\n",
    "for station in local_stations:\n",
    "    cass.execute(\n",
    "        \"\"\"\n",
    "        INSERT INTO weather.stations (id, name, date, record)\n",
    "        VALUES (%s, %s, todate(now()), {tmin: null, tmax: null})\n",
    "        \"\"\",\n",
    "        (station.id, station.name.strip())\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0eec1c55-7fdc-44cb-afd3-170c437d78cb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'AMBERG 1.3 SW'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#q2\n",
    "result = cass.execute(\"SELECT name FROM weather.stations WHERE id='US1WIMR0003'\").one()[0]\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "68b4a7f3-025f-45e4-9f7a-d51a85539731",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-9014250178872933741"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#q3\n",
    "currToken = cass.execute(\"SELECT token(id) FROM weather.stations WHERE id='USC00470273'\").one()[0]\n",
    "currToken"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0f0282cb-48b8-4e3e-8271-b42c87bfa453",
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess\n",
    "output = subprocess.check_output('nodetool ring', shell=True)\n",
    "decoded_output = output.decode('utf-8')\n",
    "split_output = decoded_output.split('\\n')\n",
    "\n",
    "tokens = []\n",
    "for x in split_output[5:-6]:\n",
    "    section = x.split()\n",
    "    tokens.append(int(section[7]))\n",
    "    \n",
    "for i in range(len(tokens)):\n",
    "    if currToken < tokens[i]:\n",
    "        vnode_token = tokens[i]\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "51fa10d2-d13d-4545-8305-47d8eda81340",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-8784482959056695711"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#q4\n",
    "vnode_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "935c006c-9f75-46df-be50-b95a74f10421",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Archive:  records.zip\n"
     ]
    }
   ],
   "source": [
    "!unzip -n records.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8eaaad76-0e93-4b93-a50e-a35a32e14457",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import first, col\n",
    "from datetime import datetime\n",
    "import grpc\n",
    "import station_pb2\n",
    "import station_pb2_grpc\n",
    "\n",
    "df = spark.read.parquet(\"records.parquet\")\n",
    "filtered_df = df.filter((col(\"element\") == \"TMAX\") | (col(\"element\") == \"TMIN\"))\n",
    "pivot_df = filtered_df.groupBy(\"station\", \"date\").pivot(\"element\").agg(first(\"value\"))\n",
    "weather_data = pivot_df.collect()\n",
    "\n",
    "channel = grpc.insecure_channel('localhost:5440')\n",
    "stub = station_pb2_grpc.StationStub(channel)\n",
    "\n",
    "for row in weather_data:\n",
    "    station_id = row[0]\n",
    "    date_string = row[1]\n",
    "    date = datetime.strptime(date_string, '%Y%m%d').strftime('%Y-%m-%d')\n",
    "    t_max = row['TMAX']\n",
    "    t_min = row['TMIN']\n",
    "    try:\n",
    "        err = stub.RecordTemps(station_pb2.RecordTempsRequest(station=station_id,date=date,tmin=int(t_min),tmax=int(t_max)))\n",
    "    except grpc.RpcError as e:\n",
    "        print(\"Failed to send data:\", e.details())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e5e66100-3fd9-41df-bbad-94dbb303a259",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "356"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#q5\n",
    "new_id = 'USW00014837'\n",
    "stub.StationMax(station_pb2.StationMaxRequest(station=new_id)).tmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "96e24280-00d8-46a2-8c1a-62b37b683100",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = (spark.read.format(\"org.apache.spark.sql.cassandra\")\n",
    ".option(\"spark.cassandra.connection.host\", \"p6-db-1,p6-db-2,p6-db-3\")\n",
    ".option(\"keyspace\", \"weather\")\n",
    ".option(\"table\", \"stations\")\n",
    ".load())\n",
    "\n",
    "df.createOrReplaceTempView(\"stations\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d39b3900-51cf-41de-8f7e-fd9e43d3705d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Table(name='stations', catalog=None, namespace=[], description=None, tableType='TEMPORARY', isTemporary=True)]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#q6\n",
    "tables = spark.catalog.listTables()\n",
    "tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a6f7821b-cff2-492d-afed-1e39f2aa1511",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'USR0000WDDG': 102.06849315068493,\n",
       " 'USW00014837': 105.62739726027397,\n",
       " 'USW00014839': 89.6986301369863,\n",
       " 'USW00014898': 102.93698630136986}"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#q7\n",
    "res = spark.sql(\"\"\"\n",
    "    SELECT id, AVG(record.tmax - record.tmin)\n",
    "    FROM stations\n",
    "    WHERE record IS NOT NULL\n",
    "    GROUP BY id\n",
    "\"\"\")\n",
    "\n",
    "res_dict = res.rdd.collectAsMap()\n",
    "res_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d8dbe0f4-d5b5-444a-b8a9-fe8cbf0df958",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Datacenter: datacenter1\n",
      "=======================\n",
      "Status=Up/Down\n",
      "|/ State=Normal/Leaving/Joining/Moving\n",
      "--  Address     Load        Tokens  Owns (effective)  Host ID                               Rack \n",
      "UN  172.22.0.2  133.07 KiB  16      100.0%            98d6bc19-ebce-434a-bbf4-b04836f5f591  rack1\n",
      "UN  172.22.0.3  133.39 KiB  16      100.0%            3b2f5122-1b49-46b7-a52a-479b2f479a25  rack1\n",
      "UN  172.22.0.4  132.96 KiB  16      100.0%            6b9500ae-0149-45ca-af77-9930922a6587  rack1\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#q8\n",
    "!nodetool status"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "4a12a3de-9b28-4ba2-a7a4-d5b2fd11e7bf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "''"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#q9\n",
    "new_id = 'USW00014837'\n",
    "stub.StationMax(station_pb2.StationMaxRequest(station=new_id)).error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "62dff3ae-670a-4ccc-8c10-055a2b64f969",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#q10\n",
    "station_id = \"US312344\"\n",
    "date = \"2022-04-25\"\n",
    "t_min = -5\n",
    "t_max = 30\n",
    "stub.RecordTemps(station_pb2.RecordTempsRequest(station=station_id,date=date,tmin=int(t_min),tmax=int(t_max)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
